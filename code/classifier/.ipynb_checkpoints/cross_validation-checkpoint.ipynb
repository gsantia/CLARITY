{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime, json, re, math, sys\n",
    "import numpy as np\n",
    "import numpy #lol\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress as lm\n",
    "import scipy\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "from __future__ import division\n",
    "% matplotlib inline\n",
    "#import the files\n",
    "sys.path.append('../API')\n",
    "import main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mu analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    counts = Counter()\n",
    "    text = text.encode(\"utf8\")\n",
    "    words = []\n",
    "    for word in re.split(\" \", text):\n",
    "        if word not in counts:\n",
    "            words.append(word)\n",
    "        counts[word] += 1\n",
    "        \n",
    "    return counts\n",
    "\n",
    "def order(counts):\n",
    "    words = counts.keys()\n",
    "    ps = np.array(counts.values())\n",
    "    ps = ps/float(sum(ps))\n",
    "    N = len(words)\n",
    "    return np.random.choice(words, size=N, replace=False, p=ps)\n",
    "\n",
    "def innovationrate(counts, reps = 2, termmax = 100):\n",
    "    \n",
    "    N = len(counts)    \n",
    "    FN = sum(counts.values())\n",
    "\n",
    "    ns = range(1,N+1)\n",
    "    Mn = [0 for n in ns]\n",
    "    \n",
    "    for rep in range(reps):\n",
    "        n = 0\n",
    "        Fn = 0\n",
    "        Msum = 0\n",
    "        for n, word in zip(ns, order(counts)):\n",
    "\n",
    "            f = counts[word]\n",
    "            Fn += f\n",
    "\n",
    "            if n == N:\n",
    "                break \n",
    "\n",
    "            ## compute In and Jn\n",
    "            In = Fn - (n - 1 + int(Msum))\n",
    "            Jn = FN - (n - 1 + int(Msum))\n",
    "\n",
    "            ## compute the average\n",
    "            ms = np.array(range(1, min([In,termmax])+1))\n",
    "\n",
    "            logfacts = np.log10(In - ms) - np.log10(Jn - ms)\n",
    "            prods = 10**np.cumsum(logfacts)\n",
    "            terms = ms*prods*(Jn - In)/(Jn - ms)\n",
    "            termsum = sum(terms)\n",
    "            Mn[n] += termsum/reps\n",
    "\n",
    "            Msum += termsum\n",
    "\n",
    "    return 1/(1 + np.array(Mn)), np.array(ns)\n",
    "\n",
    "def decayExponent(text):\n",
    "    counts = preprocess(text)\n",
    "    reps = int(round(0.5 + (5000. / len(counts))))\n",
    "    termmax = 1000\n",
    "    alphas, ns = innovationrate(counts, reps, termmax)\n",
    "    \n",
    "    ix = range(int(len(ns)*1/3.),len(ns))\n",
    "\n",
    "    x = np.log10(ns)[ix]\n",
    "    y = np.log10(alphas)[ix]\n",
    "\n",
    "    mu, b, r, p, err = lm(x,y)\n",
    "\n",
    "    return -mu, sum(counts.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMu(text):\n",
    "    if text.strip(): #in case there's no text\n",
    "        mu, numwords = decayExponent(text)\n",
    "        return mu, numwords\n",
    "    else:\n",
    "        return 0.0, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getNumLinks(text):\n",
    "    links = re.findall(r'(https?://[^\\s]+)', text)\n",
    "    return len(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deviation from thread mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def thread_deviation(comment_dict):\n",
    "    \"\"\"get the deviation from the thread's mean response time for\n",
    "    the comment\"\"\"\n",
    "    try:\n",
    "        thread_id = comment_dict['id'].split('_')[0]\n",
    "        thread_times = main.threadResponse(thread_id)\n",
    "        thread_mean = np.mean(thread_times)\n",
    "        thread_deviation = comment_dict['response'] - thread_mean\n",
    "        return thread_deviation\n",
    "    except:\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../../data/COMMENTS.json', 'r') as f2:\n",
    "    COMMENTS = json.load(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('TRAIN_STATS.json', 'r') as f:\n",
    "    ALL_STATS = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(commID, thresholds = [2086.68, 0, 0.2, 0.5, 0, 0.25, 100]):\n",
    "    #     delta_t = userFeatures[userID][0]\n",
    "    #     mu = userFeatures[userID][2]\n",
    "    #     l = userFeatures[userID][4]\n",
    "    #     c_bar = userFeatures[userID][5]\n",
    "    #     d_max = userFeatures[userID][6]\n",
    "    # i think that threshold = [avg response time,\n",
    "    #                           deviation from thread mean,\n",
    "    #                           mu lower,\n",
    "    #                           mu upper,\n",
    "    #                           mu word count,\n",
    "    #                           avg number links,\n",
    "    #                           avg comment length]\n",
    "    \n",
    "    if commID in ALL_STATS:\n",
    "        comm_stats = ALL_STATS[commID]\n",
    "        mu1, mu2, num_links, response, deviation, length = comm_stats\n",
    "    else:\n",
    "        comm = COMMENTS[commID]\n",
    "        text = comm['message']\n",
    "        mu1, mu2 = getMu(text)\n",
    "        num_links = getNumLinks(text)\n",
    "        response = comm['response']\n",
    "        deviation = thread_deviation(comm)\n",
    "        length = len(text)\n",
    "    \n",
    "    if mu1 < thresholds[2] or mu1 > thresholds[3] or num_links > thresholds[5] or length > thresholds[6]:\n",
    "        if response > thresholds[0] or deviation > thresholds[1]:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(list_commIDs):\n",
    "    \"\"\"splits the list of list_commIDs into 10 equally-sized, random samples\n",
    "    and returns a nested list representing them\"\"\"\n",
    "    random.seed(0)\n",
    "    random.shuffle(list_commIDs)\n",
    "    data_split = [[] for i in range(10)]\n",
    "    for i, comm in enumerate(list_commIDs):\n",
    "        batch_num = i % 10\n",
    "        data_split[batch_num].append(comm)\n",
    "    return data_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Create the function itself. We want to apply this to the 7 different parameters, so allow for one of the arguments\n",
    "to be the parameter:\n",
    "* 0 = response time\n",
    "* 1 = deviation from thread mean response time\n",
    "* 2 = mu lower bound\n",
    "* 3 = mu upper bound\n",
    "* 4 = mu word count\n",
    "* 5 = number links\n",
    "* 6 = comment length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'ALL_USER_STATS.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-ef4348fd321f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ALL_USER_STATS.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mall_users\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#get the bots info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../data/annotation.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'ALL_USER_STATS.json'"
     ]
    }
   ],
   "source": [
    "with open('ALL_USER_STATS.json', 'r') as f:\n",
    "    all_users = json.load(f)\n",
    "    \n",
    "#get the bots info\n",
    "with open('../../data/annotation.json', 'r') as f2:\n",
    "    annotation = json.load(f2)\n",
    "    \n",
    "BOTS = [user for bucket in annotation for user in annotation[bucket] if annotation[bucket][user] != '1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crossValidateParam(parameter, guess_range, n = 100):\n",
    "    \"\"\"performs 10-fold cross validation on the parameter which is found by using the function\n",
    "    _param_function_. _all_users_ is a dict of the annotation data.\n",
    "    _guess_range_ is a tuple of two numbers, which is the range of values that\n",
    "    we'll be scanning through for the parameter. _n_ is the size of the mesh for the interval\n",
    "    _guess_range_. returns a list of the best parameter value for each of the 10 folds\"\"\"\n",
    "    \n",
    "    data_split = split_data(ALL_STATS.keys())\n",
    "    result = [(None, None) for _ in range(10)] #list of tuples (parameter, F1 score) for each fold\n",
    "    \n",
    "    for i, fold in enumerate(data_split):\n",
    "        best_param = 0 \n",
    "        max_F1 = -1000   #stores the F1 score associated with best_param\n",
    "        copy = list(data_split)   #we don't want to mess up the data\n",
    "        test = copy.pop(i)   \n",
    "        train = [comm for fold in copy for comm in fold]  #flatten the list\n",
    "        #now start scanning the parameter values\n",
    "        for scan_num in range(n):\n",
    "            step_size = (guess_range[1] - guess_range[0]) / n  #increasing by this much each iteration\n",
    "            param_value = guess_range[0] + step_size * scan_num  #test this paramater value\n",
    "            param_F1 = cross_helper(train, parameter, param_value)\n",
    "            #check if this is better than what we already have\n",
    "            if param_F1 > max_F1:\n",
    "                #update our values\n",
    "                best_param = param_value\n",
    "                max_F1 = param_F1\n",
    "        \n",
    "        #now that we have the best param for the fold, apply it to the test data\n",
    "        fold_F1 = cross_helper(test, parameter, best_param)\n",
    "        result[i] = (best_param, fold_F1)\n",
    "    return result\n",
    "        \n",
    "def cross_helper(comments, parameter, param_value):     \n",
    "    \"\"\"takes a list of comments with a parameter and its value and returns the F1\n",
    "    score obtained by using that value of the parameter in classification on only\n",
    "    the users desired\"\"\"\n",
    "    thresholds = [float('Inf'), float('Inf'), -float('Inf'), float('Inf'),\n",
    "                  0, float('Inf'), float('Inf')]\n",
    "    if parameter in (0, 1):\n",
    "        thresholds = [float('Inf'), float('Inf'), 0, -float('Inf'), 0, 0, 0]  #just need one to set off the \"OR\" switch\n",
    "    thresholds[parameter] = param_value\n",
    "    return create_matrix(comments, thresholds) \n",
    "\n",
    "def create_matrix(comments, thresholds):\n",
    "    \"\"\"classifies each comment in _comments_ according to _thresholds_ and creates the confusion matrix. returns\n",
    "    the corresponding F1 score\"\"\"\n",
    "    confusion = Counter()\n",
    "    for comm in comments:\n",
    "        classification = classify(comm, thresholds)\n",
    "        if COMMENTS[comm]['bot']:  # bot comment\n",
    "            if classification:\n",
    "                confusion[\"tp\"] += 1 #classify says bot, is bot\n",
    "            else:\n",
    "                confusion[\"fn\"] += 1  #classify says human, is bot\n",
    "            \n",
    "        else: # human comment\n",
    "            if classification:\n",
    "                confusion[\"fp\"] += 1  #classify says bot, but human\n",
    "            else:\n",
    "                confusion[\"tn\"] += 1 #classify says human, is human\n",
    "    return eval_F1(confusion)\n",
    "\n",
    "def eval_precision(confusion_matrix):\n",
    "    \"\"\"given the Counter _confusion_matrix_, calculates and returns the\n",
    "    precision\"\"\"\n",
    "    precision = 0.0\n",
    "    try:\n",
    "        precision = confusion_matrix[\"tp\"] / (confusion_matrix[\"tp\"] + confusion_matrix[\"fp\"])\n",
    "    except:\n",
    "        pass\n",
    "    return precision\n",
    "\n",
    "def eval_recall(confusion_matrix):\n",
    "    \"\"\"given the Counter _confusion_matrix_, calculates and returns the\n",
    "    recall\"\"\"\n",
    "    recall = 0.0\n",
    "    try:\n",
    "        recall = confusion_matrix[\"tp\"] / (confusion_matrix[\"tp\"] + confusion_matrix[\"fn\"])\n",
    "    except:\n",
    "        pass\n",
    "    return recall\n",
    "\n",
    "def eval_F1(confusion_matrix):\n",
    "    \"\"\"calculates and returns the F1 score\"\"\"\n",
    "    precision = eval_precision(confusion_matrix)\n",
    "    recall = eval_recall(confusion_matrix)\n",
    "    F1 = 0.0\n",
    "    try:\n",
    "        F1 = (2 * precision * recall / (precision + recall))\n",
    "    except:\n",
    "        pass\n",
    "    return F1\n",
    "\n",
    "def total_eval(confusion_matrix):\n",
    "    \"\"\"does all 3\"\"\"\n",
    "    return eval_precision(confusion_matrix), eval_recall(confusion_matrix), eval_F1(confusion_matrix)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(30800.0, 0.6318577374195989), (44600.0, 0.6210299886119195), (30800.0, 0.6150707637383654), (30800.0, 0.6339499182081288), (30800.0, 0.6307692307692309), (30800.0, 0.6348088531187123), (44600.0, 0.6309045226130654), (30800.0, 0.6251266464032422), (41800.0, 0.6317244846656611), (30800.0, 0.6303198186854697)]\n"
     ]
    }
   ],
   "source": [
    "print crossValidateParam(1, (30000, 50000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_cross():\n",
    "    \"\"\"runs the cross-validation process on each parameter in order, and makes a list of the optimal values for\n",
    "    each. then applies these optimal values as thresholds to the full data set as a measure of the usefulness of\n",
    "    our technique\"\"\"\n",
    "    parameters = [0, 1, 5, 6]  #only want to look at these \n",
    "    intervals = [(30000, 50000), (60000, 80000), (0, .05), (0, 100)]\n",
    "    thresholds = [2000, 0, -float('Inf'), float('Inf'), 0, 0, 0]\n",
    "    for param, interval in zip(parameters, intervals):\n",
    "        results = crossValidateParam(param, interval)\n",
    "        temp_sum = 0\n",
    "        for result in results:\n",
    "            temp_sum += result[0]\n",
    "        param_result = temp_sum / 10\n",
    "        thresholds[param] = param_result  #update the parameter value\n",
    "        \n",
    "    \n",
    "    #now run this on the full set\n",
    "    confusion = Counter()\n",
    "    for comm in ALL_STATS.keys():\n",
    "        classification = classify(comm, thresholds)\n",
    "        if COMMENTS[comm]['bot']:\n",
    "            if classification:\n",
    "                confusion[\"tp\"] += 1 #classify says bot, is bot\n",
    "            else:\n",
    "                confusion[\"fn\"] += 1  #classify says human, is bot\n",
    "\n",
    "        else: # human comment\n",
    "            if classification:\n",
    "                confusion[\"fp\"] += 1  #classify says bot, but human\n",
    "            else:\n",
    "                confusion[\"tn\"] += 1 #classify says human, is human\n",
    "    #display the optimal parameters\n",
    "    print thresholds\n",
    "    return eval_precision(confusion), eval_recall(confusion), eval_F1(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37400.0, 70000.0, -inf, inf, 0, 0.0, 3.0]\n",
      "(0.48439683278993945, 0.9548902195608783, 0.6427429431285352)\n"
     ]
    }
   ],
   "source": [
    "print run_cross()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Without thread response deviation:\n",
    "(0.5024390243902439, 0.8223552894211577, 0.6237698713096139)\n",
    "[4925.0, 0, -inf, inf, 0, 0.0, 3.0]\n",
    "\n",
    "* With thread response deviation:\n",
    "(0.48874488403819916, 0.9152894211576846, 0.6372252021900446)\n",
    "* thresholds = [4925.0, 4900.0, -inf, inf, 0, 0.0, 3.0]\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Scipy Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimizeCross(thresholds = [3235.0, 1010.0, 0.004200000000000001, 162.3, 30.0]):\n",
    "    \"\"\"uses scipy.optimize to find best parameters for each fold of training data,\n",
    "    then tests it on the test data. will return a list of the best parameters found\n",
    "    by taking their averages over the 10 iterations of optimization\"\"\"\n",
    "    data_split = split_data(all_users.keys())\n",
    "    results = [[None] * 5 for _ in range(10)] #store the thresholds obtained\n",
    "    fold_F1 = [None] * 10\n",
    "    for i, fold in enumerate(data_split):\n",
    "        copy = list(data_split)   #we don't want to mess up the data\n",
    "        test = copy.pop(i)   \n",
    "        train = [comm for fold in copy for comm in fold]  #flatten the list\n",
    "        answer = scipy.optimize.minimize(log_create_matrix,\n",
    "                                         thresholds,\n",
    "                                         args = (train,),\n",
    "                                         method = 'Nelder-Mead')\n",
    "        #print answer.x\n",
    "        results[i] = answer.x\n",
    "        fold_F1[i] = optimize_matrix(results[i], test)\n",
    "    final_params = [np.mean(param) for param in zip(*results)] #take the means across the folds\n",
    "    \n",
    "    confusion = Counter()\n",
    "    for user in all_users.keys():\n",
    "        classification = optimize_classify(user, final_params)\n",
    "        if user in BOTS:  #the user is a bot\n",
    "            if classification:\n",
    "                confusion[\"tp\"] += 1 #classify says bot, is bot\n",
    "            else:\n",
    "                confusion[\"fn\"] += 1  #classify says human, is bot\n",
    "            \n",
    "        else: #the user is a human\n",
    "            if classification:\n",
    "                confusion[\"fp\"] += 1  #classify says bot, but human\n",
    "            else:\n",
    "                confusion[\"tn\"] += 1 #classify says human, is human\n",
    "    print final_params\n",
    "    print fold_F1\n",
    "    return total_eval(confusion)\n",
    "  \n",
    "\n",
    "def log_create_matrix(thresholds, users):\n",
    "    \"\"\"since we're performing a minimization procedure we'll be converting the F1 scores\n",
    "    returned by create_matrix to be the negative logs, to allow for better accuracy. this\n",
    "    is the function we'll be running the process on. temp_thresholds must be the first\n",
    "    input as per scipy's requirements, and will just be the variables we're trying to \n",
    "    optimize (the three mu values won'be used yet.)\"\"\"\n",
    "    return -math.log(optimize_matrix(thresholds, users))\n",
    "\n",
    "def optimize_matrix(thresholds, users):\n",
    "    \"\"\"the old one doesn't seem to work with the optimization function, since\n",
    "    we're trying to add the mu values. this version of the function will be the\n",
    "    same, but we won't require any setting of the mu values\"\"\"\n",
    "    confusion = Counter()\n",
    "    for user in users:\n",
    "        classification = optimize_classify(user, thresholds)\n",
    "        if user in BOTS:  #the user is a bot\n",
    "            if classification:\n",
    "                confusion[\"tp\"] += 1 #classify says bot, is bot\n",
    "            else:\n",
    "                confusion[\"fn\"] += 1  #classify says human, is bot\n",
    "            \n",
    "        else: #the user is a human\n",
    "            if classification:\n",
    "                confusion[\"fp\"] += 1  #classify says bot, but human\n",
    "            else:\n",
    "                confusion[\"tn\"] += 1 #classify says human, is human\n",
    "    return eval_F1(confusion)\n",
    "\n",
    "def optimize_classify(userID, thresholds):\n",
    "    \"\"\"the mu values are messing the optimization up. this is exactly the same as the normal\n",
    "    classify function except the mu values are completely taken out, for now.\"\"\"\n",
    "\n",
    "    if userID in all_users:\n",
    "        delta_t = all_users[userID][4]\n",
    "        l = all_users[userID][2]\n",
    "        c_bar = all_users[userID][6]\n",
    "        d_max = all_users[userID][3]\n",
    "        thread_avg = all_users[userID][5]\n",
    "    else:\n",
    "        delta_t = avg_response(userID)\n",
    "        l = getNumLinks(userID, all_comments)\n",
    "        c_bar = comment_length(userID)\n",
    "        d_max = maxDailyComments(userID)\n",
    "        thread_avg = thread_deviation(userID)\n",
    "    \n",
    "    if l > thresholds[2] or d_max > thresholds[4] or c_bar > thresholds[3]:\n",
    "        if delta_t > thresholds[0] or thread_avg > thresholds[1]:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3176.8488895278083, 989.54603895500804, 0.0041883628274073624, 174.42818973379582, 30.156292945920008]\n",
      "[0.5000000000000001, 0.3783783783783784, 0.5909090909090909, 0.7037037037037038, 0.5714285714285714, 0.5641025641025641, 0.42553191489361697, 0.625, 0.6341463414634146, 0.48979591836734687]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.42955326460481097, 0.8169934640522876, 0.563063063063063)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizeCross()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Results of optimization:\n",
    "(recall, precision, F1)\n",
    "* threshold from just parameter scan: (0.41946308724832215, 0.8169934640522876, 0.5543237250554325)\n",
    "* after scipy.optimize: (0.42955326460481097, 0.8169934640522876, 0.563063063063063)\n",
    "\n",
    "Looks like the optimization just increased precision a bit. Recall was exactly the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
