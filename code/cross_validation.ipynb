{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime, json, re, math, sys\n",
    "import numpy as np\n",
    "import numpy #lol\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress as lm\n",
    "import scipy\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "from __future__ import division\n",
    "% matplotlib inline\n",
    "#import the files\n",
    "sys.path.append('../API')\n",
    "import main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mu analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    counts = Counter()\n",
    "    text = text.encode(\"utf8\")\n",
    "    words = []\n",
    "    for word in re.split(\" \", text):\n",
    "        if word not in counts:\n",
    "            words.append(word)\n",
    "        counts[word] += 1\n",
    "        \n",
    "    return counts\n",
    "\n",
    "def order(counts):\n",
    "    words = counts.keys()\n",
    "    ps = np.array(counts.values())\n",
    "    ps = ps/float(sum(ps))\n",
    "    N = len(words)\n",
    "    return np.random.choice(words, size=N, replace=False, p=ps)\n",
    "\n",
    "def innovationrate(counts, reps = 2, termmax = 100):\n",
    "    \n",
    "    N = len(counts)    \n",
    "    FN = sum(counts.values())\n",
    "\n",
    "    ns = range(1,N+1)\n",
    "    Mn = [0 for n in ns]\n",
    "    \n",
    "    for rep in range(reps):\n",
    "        n = 0\n",
    "        Fn = 0\n",
    "        Msum = 0\n",
    "        for n, word in zip(ns, order(counts)):\n",
    "\n",
    "            f = counts[word]\n",
    "            Fn += f\n",
    "\n",
    "            if n == N:\n",
    "                break \n",
    "\n",
    "            ## compute In and Jn\n",
    "            In = Fn - (n - 1 + int(Msum))\n",
    "            Jn = FN - (n - 1 + int(Msum))\n",
    "\n",
    "            ## compute the average\n",
    "            ms = np.array(range(1, min([In,termmax])+1))\n",
    "\n",
    "            logfacts = np.log10(In - ms) - np.log10(Jn - ms)\n",
    "            prods = 10**np.cumsum(logfacts)\n",
    "            terms = ms*prods*(Jn - In)/(Jn - ms)\n",
    "            termsum = sum(terms)\n",
    "            Mn[n] += termsum/reps\n",
    "\n",
    "            Msum += termsum\n",
    "\n",
    "    return 1/(1 + np.array(Mn)), np.array(ns)\n",
    "\n",
    "def decayExponent(text):\n",
    "    counts = preprocess(text)\n",
    "    reps = int(round(0.5 + (5000. / len(counts))))\n",
    "    termmax = 1000\n",
    "    alphas, ns = innovationrate(counts, reps, termmax)\n",
    "    \n",
    "    ix = range(int(len(ns)*1/3.),len(ns))\n",
    "\n",
    "    x = np.log10(ns)[ix]\n",
    "    y = np.log10(alphas)[ix]\n",
    "\n",
    "    mu, b, r, p, err = lm(x,y)\n",
    "\n",
    "    return -mu, sum(counts.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_comments = {user : main.userText(user) for user in main.COMMENTS_BY_USER}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMu(userID, all_comments):\n",
    "    comments = all_comments[userID]\n",
    "    total_text = \"\"\n",
    "    for comment in comments:\n",
    "        total_text = total_text + comment + \" \"\n",
    "    if total_text.strip(): #in case there's no text\n",
    "        mu, numwords = decayExponent(total_text)\n",
    "        return mu, numwords\n",
    "    else:\n",
    "        return 0.0, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getNumLinks(userID, all_comments):\n",
    "    numlinks = []\n",
    "    comments = all_comments[userID]\n",
    "    for comment in comments:\n",
    "        links = re.findall(r'(https?://[^\\s]+)', comment)\n",
    "        numlink = len(links)\n",
    "        numlinks.append(numlink)\n",
    "    avg_numlinks = np.mean(numlinks)\n",
    "    return avg_numlinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User's max daily comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxDailyComments(UID):\n",
    "    days = Counter()\n",
    "    for comment in main.COMMENTS_BY_USER[UID]:\n",
    "        date = comment['time'][0:10]\n",
    "        days[date] += 1\n",
    "    return(max(days.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average response time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avg_response(userID, choice = \"All\"):\n",
    "    \"\"\"get the avg response time for _userID\"\"\"\n",
    "    comment_times = main.userResponse(userID, choice)\n",
    "    if not comment_times:\n",
    "        return None\n",
    "    else:\n",
    "        return np.mean(comment_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deviation from thread mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def thread_deviation(userID, choice = \"All\"):\n",
    "    \"\"\"get the average deviation from the thread's mean response time for\n",
    "    comments made by _userID_\"\"\"\n",
    "    deviations = []\n",
    "    thread_dict = defaultdict(list)\n",
    "    threads = main.userThreads(userID, choice)\n",
    "    comment_times = main.userResponse(userID, choice)\n",
    "    if comment_times:\n",
    "        for thread, comment_time in zip(threads, comment_times):\n",
    "            #need to organize this by thread\n",
    "            thread_dict[thread].append(comment_time)\n",
    "        for threadID in thread_dict:\n",
    "            thread_deviations = []\n",
    "            thread_times = main.threadResponse(threadID, choice)\n",
    "            thread_mean = np.mean(thread_times)\n",
    "            for response in thread_dict[threadID]:\n",
    "                thread_deviations.append(response - thread_mean)\n",
    "            avg_thread_deviation = np.mean(thread_deviations)\n",
    "            #now add to the total deviations\n",
    "            deviations.append(avg_thread_deviation)\n",
    "        return np.mean(deviations)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average comment length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def comment_length(userID, choice = \"All\"):\n",
    "    \"\"\"get the avg length of comments made by _userID_\"\"\"\n",
    "    comments = main.userText(userID, choice)\n",
    "    lengths = [len(comment) for comment in comments]\n",
    "    return np.mean(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(userID, thresholds = [2086.68, 0, 0.2, 0.5, 0, 0.25, 100, 10]):\n",
    "    #     delta_t = userFeatures[userID][0]\n",
    "    #     mu = userFeatures[userID][2]\n",
    "    #     l = userFeatures[userID][4]\n",
    "    #     c_bar = userFeatures[userID][5]\n",
    "    #     d_max = userFeatures[userID][6]\n",
    "    # i think that threshold = [avg response time,\n",
    "    #                           deviation from thread mean,\n",
    "    #                           mu lower,\n",
    "    #                           mu upper,\n",
    "    #                           mu word count,\n",
    "    #                           avg number links,\n",
    "    #                           avg comment length,\n",
    "    #                           max comments per day]\n",
    "    # but not 100% sure???\n",
    "    if userID in all_users:\n",
    "        delta_t = all_users[userID][4]\n",
    "        mu = all_users[userID][0]\n",
    "        l = all_users[userID][2]\n",
    "        c_bar = all_users[userID][6]\n",
    "        d_max = all_users[userID][3]\n",
    "        thread_avg = all_users[userID][5]\n",
    "    else:\n",
    "        delta_t = avg_response(userID)\n",
    "        mu, _ = getMu(userID, all_comments)\n",
    "        l = getNumLinks(userID, all_comments)\n",
    "        c_bar = comment_length(userID)\n",
    "        d_max = maxDailyComments(userID)\n",
    "        thread_avg = thread_deviation(userID)\n",
    "    \n",
    "    if mu < thresholds[2] or mu > thresholds[3] or l > thresholds[5] or d_max > thresholds[7] or c_bar > thresholds[6]:\n",
    "        if delta_t > thresholds[0] or thread_avg > thresholds[1]:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    else:\n",
    "        return False\n",
    "        \n",
    "#     if mu > thresholds[2] and mu < thresholds[3]:\n",
    "#         if l > thresholds[5]:\n",
    "#             if delta_t > thresholds[0]:\n",
    "#                 return False\n",
    "#             else:\n",
    "#                 return True\n",
    "#         elif d_max > thresholds[7]:\n",
    "#             if delta_t > thresholds[0]:\n",
    "#                 return False\n",
    "#             else:\n",
    "#                 return True\n",
    "#         elif c_bar > thresholds[6]:\n",
    "#             if delta_t > thresholds[0]:\n",
    "#                 return False\n",
    "#             else:\n",
    "#                 return True\n",
    "#         else:\n",
    "#             return False\n",
    "#     elif delta_t > thresholds[0]:\n",
    "#         return False\n",
    "#     else:\n",
    "#         return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_users(list_users):\n",
    "    \"\"\"splits the list of userIDs _users_ into 10 equally-sized, random samples\n",
    "    and returns a nested list representing them\"\"\"\n",
    "    random.seed(0)\n",
    "    random.shuffle(list_users)\n",
    "    data_split = [[] for i in range(10)]\n",
    "    for i, user in enumerate(list_users):\n",
    "        batch_num = i % 10\n",
    "        data_split[batch_num].append(user)\n",
    "    return data_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Create the function itself. We want to apply this to the 7 different parameters, so allow for one of the arguments\n",
    "to be the parameter:\n",
    "* 0 = average response time\n",
    "* 1 = deviation from thread mean response time\n",
    "* 2 = mu lower bound\n",
    "* 3 = mu upper bound\n",
    "* 4 = mu word count\n",
    "* 5 = average number links\n",
    "* 6 = avg comment length\n",
    "* 7 = max comments/day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('ALL_USER_STATS.json', 'r') as f:\n",
    "    all_users = json.load(f)\n",
    "    \n",
    "#get the bots info\n",
    "with open('../../data/annotation.json', 'r') as f2:\n",
    "    annotation = json.load(f2)\n",
    "    \n",
    "BOTS = [user for bucket in annotation for user in annotation[bucket] if annotation[bucket][user] != '1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crossValidateParam(parameter, guess_range, n = 100):\n",
    "    \"\"\"performs 10-fold cross validation on the parameter which is found by using the function\n",
    "    _param_function_. _all_users_ is a dict of the annotation data.\n",
    "    _guess_range_ is a tuple of two numbers, which is the range of values that\n",
    "    we'll be scanning through for the parameter. _n_ is the size of the mesh for the interval\n",
    "    _guess_range_. returns a list of the best parameter value for each of the 10 folds\"\"\"\n",
    "    \n",
    "    data_split = split_users(all_users.keys())\n",
    "    result = [(None, None) for _ in range(10)] #list of tuples (parameter, F1 score) for each fold\n",
    "    \n",
    "    for i, fold in enumerate(data_split):\n",
    "        best_param = 0 \n",
    "        max_F1 = -1000   #stores the F1 score associated with best_param\n",
    "        copy = list(data_split)   #we don't want to mess up the data\n",
    "        test = copy.pop(i)   \n",
    "        train = [user for fold in copy for user in fold]  #flatten the list\n",
    "        #now start scanning the parameter values\n",
    "        for scan_num in range(n):\n",
    "            step_size = (guess_range[1] - guess_range[0]) / n  #increasing by this much each iteration\n",
    "            param_value = guess_range[0] + step_size * scan_num  #test this paramater value\n",
    "            param_F1 = cross_helper(train, parameter, param_value)\n",
    "            #check if this is better than what we already have\n",
    "            if param_F1 > max_F1:\n",
    "                #update our values\n",
    "                best_param = param_value\n",
    "                max_F1 = param_F1\n",
    "        \n",
    "        #now that we have the best param for the fold, apply it to the test data\n",
    "        fold_F1 = cross_helper(test, parameter, best_param)\n",
    "        result[i] = (best_param, fold_F1)\n",
    "    return result\n",
    "        \n",
    "def cross_helper(users, parameter, param_value):     \n",
    "    \"\"\"takes a list of users with a parameter and its value and returns the F1\n",
    "    score obtained by using that value of the parameter in classification on only\n",
    "    the users desired\"\"\"\n",
    "    #TODO: make this give you the ability to input multiple parameters, not just one\n",
    "    # so that we can use optimization in scipy or something\n",
    "    thresholds = [float('Inf'), float('Inf'), -float('Inf'), float('Inf'),\n",
    "                  0, float('Inf'), float('Inf'), float('Inf')]\n",
    "    if parameter in [0, 1]:\n",
    "        thresholds = [float('Inf'), float('Inf'), 0, -float('Inf'), 0, 0, 0, 0]  #just need one to set off the \"OR\" switch\n",
    "    thresholds[parameter] = param_value\n",
    "    return create_matrix(users, thresholds) \n",
    "\n",
    "def create_matrix(users, thresholds):\n",
    "    \"\"\"classifies each user in _users_ according to _thresholds_ and creates the confusion matrix. returns\n",
    "    the corresponding F1 score\"\"\"\n",
    "    confusion = Counter()\n",
    "    for user in users:\n",
    "        classification = classify(user, thresholds)\n",
    "        if user in BOTS:  #the user is a bot\n",
    "            if classification:\n",
    "                confusion[\"tp\"] += 1 #classify says bot, is bot\n",
    "            else:\n",
    "                confusion[\"fn\"] += 1  #classify says human, is bot\n",
    "            \n",
    "        else: #the user is a human\n",
    "            if classification:\n",
    "                confusion[\"fp\"] += 1  #classify says bot, but human\n",
    "            else:\n",
    "                confusion[\"tn\"] += 1 #classify says human, is human\n",
    "    return eval_F1(confusion)\n",
    "\n",
    "def eval_precision(confusion_matrix):\n",
    "    \"\"\"given the Counter _confusion_matrix_, calculates and returns the\n",
    "    precision\"\"\"\n",
    "    precision = 0.0\n",
    "    try:\n",
    "        precision = confusion_matrix[\"tp\"] / (confusion_matrix[\"tp\"] + confusion_matrix[\"fp\"])\n",
    "    except:\n",
    "        pass\n",
    "    return precision\n",
    "\n",
    "def eval_recall(confusion_matrix):\n",
    "    \"\"\"given the Counter _confusion_matrix_, calculates and returns the\n",
    "    recall\"\"\"\n",
    "    recall = 0.0\n",
    "    try:\n",
    "        recall = confusion_matrix[\"tp\"] / (confusion_matrix[\"tp\"] + confusion_matrix[\"fn\"])\n",
    "    except:\n",
    "        pass\n",
    "    return recall\n",
    "\n",
    "def eval_F1(confusion_matrix):\n",
    "    \"\"\"calculates and returns the F1 score\"\"\"\n",
    "    precision = eval_precision(confusion_matrix)\n",
    "    recall = eval_recall(confusion_matrix)\n",
    "    F1 = 0.0\n",
    "    try:\n",
    "        F1 = (2 * precision * recall / (precision + recall))\n",
    "    except:\n",
    "        pass\n",
    "    return F1\n",
    "\n",
    "def total_eval(confusion_matrix):\n",
    "    \"\"\"does all 3\"\"\"\n",
    "    return eval_precision(confusion_matrix), eval_recall(confusion_matrix), eval_F1(confusion_matrix)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_cross():\n",
    "    \"\"\"runs the cross-validation process on each parameter in order, and makes a list of the optimal values for\n",
    "    each. then applies these optimal values as thresholds to the full data set as a measure of the usefulness of\n",
    "    our technique\"\"\"\n",
    "    parameters = [0, 1, 5, 6, 7]  #only want to look at these \n",
    "    intervals = [(0, 5000), (-5000, 5000), (0, .1), (0, 300), (0, 100)]\n",
    "    thresholds = [2000, 0, -float('Inf'), float('Inf'), 0, 0, 0, 0]\n",
    "    for param, interval in zip(parameters, intervals):\n",
    "        results = crossValidateParam(param, interval)\n",
    "        temp_sum = 0\n",
    "        for result in results:\n",
    "            temp_sum += result[0]\n",
    "        param_result = temp_sum / 10\n",
    "        thresholds[param] = param_result  #update the parameter value\n",
    "        \n",
    "    \n",
    "    #now run this on the full set\n",
    "    confusion = Counter()\n",
    "    for user in all_users.keys():\n",
    "        classification = classify(user, thresholds)\n",
    "        if user in BOTS:\n",
    "            if classification:\n",
    "                confusion[\"tp\"] += 1 #classify says bot, is bot\n",
    "            else:\n",
    "                confusion[\"fn\"] += 1  #classify says human, is bot\n",
    "\n",
    "        else: #the user is a human\n",
    "            if classification:\n",
    "                confusion[\"fp\"] += 1  #classify says bot, but human\n",
    "            else:\n",
    "                confusion[\"tn\"] += 1 #classify says human, is human\n",
    "    #display the optimal parameters\n",
    "    print thresholds\n",
    "    return eval_precision(confusion), eval_recall(confusion), eval_F1(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1300.0, 0.205607476635514), (-1700.0, 0.11764705882352942), (1300.0, 0.2522522522522523), (-1700.0, 0.2), (1300.0, 0.2727272727272727), (1300.0, 0.2522522522522523), (1300.0, 0.2037037037037037), (3000.0, 0.3157894736842105), (3000.0, 0.24778761061946902), (1000.0, 0.2727272727272727)]\n"
     ]
    }
   ],
   "source": [
    "print crossValidateParam(1, (-5000,5000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Without thread response deviation:\n",
    "(0.41721854304635764, 0.8235294117647058, 0.553846153846154)\n",
    "\n",
    "* With thread response deviation:\n",
    "(0.41946308724832215, 0.8169934640522876, 0.5543237250554325)\n",
    "* thresholds = [3235.0, 1010.0, -inf, inf, 0, 0.004200000000000001, 162.3, 30.0]\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "* histogram of deviation from mean thread response time, figure out how to use it (indicator or not)\n",
    "* actually just do histograms for all the parameters\n",
    "* then create a scatterplot matrix for all the parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Scipy Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimizeCross(thresholds = [3235.0, 1010.0, 0.004200000000000001, 162.3, 30.0]):\n",
    "    \"\"\"uses scipy.optimize to find best parameters for each fold of training data,\n",
    "    then tests it on the test data. will return a list of the best parameters found\n",
    "    by taking their averages over the 10 iterations of optimization\"\"\"\n",
    "    data_split = split_users(all_users.keys())\n",
    "    results = [[None] * 5 for _ in range(10)] #store the thresholds obtained\n",
    "    fold_F1 = [None] * 10\n",
    "    for i, fold in enumerate(data_split):\n",
    "        #TODO: implement scipy.optimize.minimize here!\n",
    "        copy = list(data_split)   #we don't want to mess up the data\n",
    "        test = copy.pop(i)   \n",
    "        train = [user for fold in copy for user in fold]  #flatten the list\n",
    "        answer = scipy.optimize.minimize(log_create_matrix,\n",
    "                                         thresholds,\n",
    "                                         args = (train,),\n",
    "                                         method = 'Nelder-Mead')\n",
    "        #print answer.x\n",
    "        results[i] = answer.x\n",
    "        fold_F1[i] = optimize_matrix(results[i], test)\n",
    "    final_params = [np.mean(param) for param in zip(*results)] #take the means across the folds\n",
    "    \n",
    "    confusion = Counter()\n",
    "    for user in all_users.keys():\n",
    "        classification = optimize_classify(user, final_params)\n",
    "        if user in BOTS:  #the user is a bot\n",
    "            if classification:\n",
    "                confusion[\"tp\"] += 1 #classify says bot, is bot\n",
    "            else:\n",
    "                confusion[\"fn\"] += 1  #classify says human, is bot\n",
    "            \n",
    "        else: #the user is a human\n",
    "            if classification:\n",
    "                confusion[\"fp\"] += 1  #classify says bot, but human\n",
    "            else:\n",
    "                confusion[\"tn\"] += 1 #classify says human, is human\n",
    "    print final_params\n",
    "    print fold_F1\n",
    "    return total_eval(confusion)\n",
    "  \n",
    "\n",
    "def log_create_matrix(thresholds, users):\n",
    "    \"\"\"since we're performing a minimization procedure we'll be converting the F1 scores\n",
    "    returned by create_matrix to be the negative logs, to allow for better accuracy. this\n",
    "    is the function we'll be running the process on. temp_thresholds must be the first\n",
    "    input as per scipy's requirements, and will just be the variables we're trying to \n",
    "    optimize (the three mu values won'be used yet.)\"\"\"\n",
    "    return -math.log(optimize_matrix(thresholds, users))\n",
    "\n",
    "def optimize_matrix(thresholds, users):\n",
    "    \"\"\"the old one doesn't seem to work with the optimization function, since\n",
    "    we're trying to add the mu values. this version of the function will be the\n",
    "    same, but we won't require any setting of the mu values\"\"\"\n",
    "    confusion = Counter()\n",
    "    for user in users:\n",
    "        classification = optimize_classify(user, thresholds)\n",
    "        if user in BOTS:  #the user is a bot\n",
    "            if classification:\n",
    "                confusion[\"tp\"] += 1 #classify says bot, is bot\n",
    "            else:\n",
    "                confusion[\"fn\"] += 1  #classify says human, is bot\n",
    "            \n",
    "        else: #the user is a human\n",
    "            if classification:\n",
    "                confusion[\"fp\"] += 1  #classify says bot, but human\n",
    "            else:\n",
    "                confusion[\"tn\"] += 1 #classify says human, is human\n",
    "    return eval_F1(confusion)\n",
    "\n",
    "def optimize_classify(userID, thresholds):\n",
    "    \"\"\"the mu values are messing the optimization up. this is exactly the same as the normal\n",
    "    classify function except the mu values are completely taken out, for now.\"\"\"\n",
    "\n",
    "    if userID in all_users:\n",
    "        delta_t = all_users[userID][4]\n",
    "        l = all_users[userID][2]\n",
    "        c_bar = all_users[userID][6]\n",
    "        d_max = all_users[userID][3]\n",
    "        thread_avg = all_users[userID][5]\n",
    "    else:\n",
    "        delta_t = avg_response(userID)\n",
    "        l = getNumLinks(userID, all_comments)\n",
    "        c_bar = comment_length(userID)\n",
    "        d_max = maxDailyComments(userID)\n",
    "        thread_avg = thread_deviation(userID)\n",
    "    \n",
    "    if l > thresholds[2] or d_max > thresholds[4] or c_bar > thresholds[3]:\n",
    "        if delta_t > thresholds[0] or thread_avg > thresholds[1]:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3176.8488895278083, 989.54603895500804, 0.0041883628274073624, 174.42818973379582, 30.156292945920008]\n",
      "[0.5000000000000001, 0.3783783783783784, 0.5909090909090909, 0.7037037037037038, 0.5714285714285714, 0.5641025641025641, 0.42553191489361697, 0.625, 0.6341463414634146, 0.48979591836734687]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.42955326460481097, 0.8169934640522876, 0.563063063063063)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizeCross()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Results of optimization:\n",
    "(recall, precision, F1)\n",
    "* threshold from just parameter scan: (0.41946308724832215, 0.8169934640522876, 0.5543237250554325)\n",
    "* after scipy.optimize: (0.42955326460481097, 0.8169934640522876, 0.563063063063063)\n",
    "\n",
    "Looks like the optimization just increased precision a bit. Recall was exactly the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
